<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 1</title>
    <style>
        ul {
            display: table; 
            margin: 0 auto;
            text-align: left;
        }

        li {
            margin: 10px 0;
        }
        figure {
            margin: 0;
        }
        .toc {
            list-style-type: decimal;
            padding-left: 20px;
        }
        .toc a {
            text-decoration: none;
        }
        body {
            font-family: 'Times New Roman', Times, serif;
            max-width: 800px;
            margin: 20px auto;
            line-height: 1.6;
            padding: 20px;
        }
        .algorithm {
            background-color: #f8f9fa;
            padding: 20px;
            border: 1px solid #dee2e6;
            border-radius: 4px;
        }
        .algorithm-title {
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 15px;
        }
        .algorithm-block {
            margin-left: 20px;
        }
        .comment {
            color: #6c757d;
            font-style: italic;
        }
        .keyword {
            font-weight: bold;
        }
        .equation {
            margin: 10px 0;
        }
    </style>

    <!-- MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]],
            },
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>
<body>
    <h1>Table of Contents</h1>
    <ul class="toc">
        <li><a href="#overview">Overview of Project</a></li>
        <li><a href="#data-generation">Data Generation</a></li>
        <li><a href="#NN">Naive NN</a></li>
    </ul>

    <h2 id="overview">Overview of Project</h2>

    <h4>Project Inspiration: <a href="https://ieeexplore.ieee.org/document/9024543" target="_blank">https://ieeexplore.ieee.org/document/9024543</a></h4>
    <p>
        Millimeter-wave (mmWave) small cell networks play an important role in 5G wireless communication systems. 
        By densely deploying a large number of mmWave small cell base stations (SBSs), thousands of connections 
        and high transmission rates are supported to provide a variety of local services [1], [2]. 
        The SBS provides short-range communications to mobile terminals (MTs) to reduce the propagation loss of 
        signal transmission [3]. With the help of mmWave, multiple SBSs can utilize a large number of antennas 
        to form directional analog beams to MTs and provide concurrent transmissions simultaneously. However, 
        as the number of SBSs and MTs increases, it becomes increasingly difficult to use traditional signal 
        processing methods to improve performance.
    </p>

    <p>
        For our project, we attempt to utilize machine learning (ML) to show a novel ML-based method for concurrent transmission in mmWave small cell networks.
    </p>

    <p>In the original paper, the authors follow a 3-step process:</p>
    <ul>
        <li>
            <strong>1. Random Distribution Modeling:</strong> 
            The large random distribution of SBSs is modeled by a heterogeneous Poisson point process (HPPP), 
            where the average sum rate (ASR) of MT under concurrent transmission can be obtained.
        </li>
        <li>
            <strong>2. Machine Learning for Beam Selection:</strong> 
            Downlink SBS conditions are established as a large database for machine learning training. An iterative 
            support vector machine (SVM) classifier is proposed for analog beam selection of each SBS.
        </li>
        <li>
            <strong>3. Iterative SMO Algorithm:</strong> 
            An iterative sequential minimal optimization (SMO) training algorithm is proposed, enabling SBSs to perform 
            highly efficient and low-complexity analog beam selection during concurrent transmissions.
        </li>
    </ul>

    <p>
        This process runs faster than traditional 
        <a href="https://www.mathworks.com/help/lte/ug/channel-estimation.html" target="_blank">channel estimation algorithms</a>
        on average.
    </p>

    <h4 style="text-decoration: underline;">For our project, we want to compare a naive Neural Network (NN) implementation, the paper's SMO algorithm, and our own Reinforced Learning (RL) approach.</h4>

    <h2> Terminology </h2>
    
    <ul>
      <li>
          <strong>1. CodeBook :</strong> 
          <p>
            A <strong>codebook</strong> in beamforming is a collection of predefined 
            <strong>candidate beamforming vectors</strong>. Each vector represents a specific configuration 
            of phase shifts across the antennas of the SBS. These configurations determine the direction 
            and focus of the transmitted signal.
        </p>
        <h2>Mathematical Definition</h2>
        <p>The codebook $C$ contains $N_C$ candidate vectors,
        \(C = \{ c_{1}^S, c_{2}^S, \dots , c_{N_C}^S \} \), where <p style="text-align: center";>
            \(c_i^S \in \mathbb{C}^{N_{SBS} \times 1} \text{ for } i = 1, 2, \dots , N_C\)

        </p>
        <ul>
            <li>
                Each $c_i^{S}$ is a normalized beamforming vector:
                <p style="text-align: center";>
                    $c_i^S = \frac{1}{\sqrt{N_{SBS}}} \cdot [e^{j\phi_1}, e^{j\phi_2}, \dots , e^{j\phi_{N_{SBS}}}]^\top$
                </p>
            </li>
            $\phi_k$: Phase shift for the k-th antenna.</li> <br>
            $N_{SBS}$: Number of antennas at the SBS.</li>
        </ul>
        <p>
            Each vector in the codebook points the beam in a different direction, enabling the SBS to focus 
            its transmission on specific targets (MTs).
        </p>
      </li>
  </ul>

    <h2 id="data-generation">Data Generation</h2>

    <p>The first <i>(and typically most important)</i> step of any ML-based project is to collect lots of <a href="https://www.linkedin.com/pulse/what-good-quality-training-dataset-machine-learning-tagx" target="_blank">good</a> data.</p>

    <p> Problem is... we don't have access to this data, and most papers on beamforming do not share their data.
      As a result, we had to implement our own data generation script that follows the paper's implementaiton.

      This meant curating a large dataset that followed a HPPP.
    </p>

    <p> </p>
    <body>
      <div class="algorithm">
          <div class="algorithm-title">Algorithm 1: mmWave Channel Model</div>
          
          <div class="keyword">Constants:</div>
          <div class="algorithm-block">
              λ_S: SBS density in R² plane<br>
              R: Maximum communication radius<br>
              N_SBS: Number of SBS antennas<br>
              N_MT: Number of MT antennas<br>
              L: Number of propagation paths<br>
              N_C: Number of candidate vectors in codebook (N_C > 2)<br>
              P_S: Transmit power of SBS<br>
              D_MT, D_S: Antenna spacing at MT and SBS (λ/2)<br>
              σ = 2π/λ: Wave number
          </div>
  
          <div class="keyword">Procedure:</div>
          <div class="algorithm-block">
              1. Calculate number of SBS:<br>
              <div class="equation">N_S = ⌊λ_S π R²⌋</div>
              
              2. Generate data streams d_S,k ∈ C for k = 1,...,N_S<br>
              
              3. For each SBS k, select beam c_S,k from codebook:<br>
              <div class="equation">s_SBS,k = c_S,k d_S,k, (c_S,k ∈ C^(N_SBS×1))</div>
              
              4. Generate channel matrices H_S,k:<br>
              <div class="equation">H_S,k = γ ∑(l=1 to L) α_S,k,l a_MT(φ_MT,k,l)[a_S,k(φ_S,k,l)]^H</div>
              where:
              <div class="algorithm-block">
                  γ = √(N_SBS N_MT / L)<br>
                  α_S,k,l ∼ CN(0,1)<br>
                  Array steering vectors:<br>
                  <div class="equation">a_MT(φ_MT,k,l) = [1, e^(jσD_MT sin(φ_MT,k,l)), ..., e^(jσD_MT(N_MT-1)sin(φ_MT,k,l))]^T/√N_MT</div>
                  <div class="equation">a_S(φ_S,k,l) = [1, e^(jσD_S sin(φ_S,k,l)), ..., e^(jσD_S(N_SBS-1)sin(φ_S,k,l))]^T/√N_SBS</div>
              </div>
              
              5. Generate receive phase vector:<br>
              <div class="equation">g_MT = [e^(jθ₁), ..., e^(jθ_N_MT)] ∈ C^(1×N_MT)</div>
              
              6. Calculate received signal:<br>
              <div class="equation">y_MT = g_MT ∑(k=1 to N_S) H_S,k c_S,k d_S,k + g_MT n</div>
              
              7. Define channel matrix G:<br>
              <div class="equation">G = g_MT[H_S,1c_S,1 ⋯ H_S,N_Sc_S,N_S]</div>
              
              8. Apply Zero-Forcing equalization:<br>
              <div class="equation">y_MT,ZF = [d_S,1, ⋯, d_S,N_S]^T + (G^H G)^(-1) G^H g_MT n</div>
          </div>
      </div>
  </body>

  <h2 id="SVM">Why not use a normal SVM?</h2>

  <p> One of the main problems we face here is that small base stations are very <b> densely</b> deployed, and their placements are changing with every snapshot. </p>

  <p>This leads us to create training samples that use multiple snapshots of SBSs with the same density, as it allows us to improve our variance in our samples.</p>

  <p>Each training sample will contain the following, assuming we have $L$ propagation paths: </p>

  <ul>
    <li>Transmit power</li>
    <li>Path loss</li>
    <li>$2L$ <a href="https://en.wikipedia.org/wiki/Azimuth" target="_blank">azimuth angles</a>, one for the angle of arrival and one for the angle of departure for each path.</li>
    <li>$2L$ real and imaginary parts for the <a href="https://en.wikipedia.org/wiki/Complex_gain" target="_blank">complex gain</a>.</li>
  </ul>

  <p>We normalize our samples in order to prevent any inconsistencies from differing range values. For instance, all angles will be between $0$ and $2\pi$, all power will be measured in dB, etc.</p>

  <p>In total, every sample will be formatted as a vector of dimension $1 \times (4L + 2)$.</p>

  <p>If we were to use a traditional SVM, then we'd end up using $N_C$ separate classifiers in order to find hyperplanes that can cleanly separate our data. However, our data can be <b>incredibly imbalanced</b> (such as some beams having much fewer samples than other beams), which can introduce heavy bias and inaccurate predictions.</p>

  <p>Therefore, by using a <b>data-driven iterative SVM classifier</b>, we can better address the issue of imbalanced data and ensure better prediction accuracy for analog beam selection.</p> Specifically, we can perform resampling/reweighting with our samples deploy a more iterative process with each learning step to improve refinement.

  <h2 id="NN">Naive NN</h2>

  <p> Firstly, we implement a Naive NN to train the data.</p>


</html>
