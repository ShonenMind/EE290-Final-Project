<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 1</title>
    <style>
        ul {
            display: table; 
            margin: 0 auto;
            text-align: left;
        }

        li {
            margin: 10px 0;
        }
        figure {
            margin: 0;
        }
        .toc {
            list-style-type: decimal;
            padding-left: 1; /* Removes unnecessary padding */
            margin-left: 0; /* Ensures alignment to the left */
            text-align: left; /* Aligns the text */
        }
        .toc a {
            text-decoration: none;
        }
        body {
            font-family: 'Times New Roman', Times, serif;
            max-width: 800px;
            margin: 20px auto;
            line-height: 1.6;
            padding: 20px;
        }
        .algorithm {
            background-color: #f8f9fa;
            padding: 20px;
            border: 1px solid #dee2e6;
            border-radius: 4px;
        }
        .algorithm-title {
            font-weight: bold;
            font-size: 1.2em;
            margin-bottom: 15px;
        }
        .algorithm-block {
            margin-left: 20px;
        }
        .comment {
            color: #6c757d;
            font-style: italic;
        }
        .keyword {
            font-weight: bold;
        }
        .equation {
            margin: 10px 0;
        }
        .title {
            text-align: center;
            font-size: 2em;
            margin: 20px 0;
            font-weight: bold;
        }
    </style>

    <!-- MathJax -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]],
            },
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

</head>





<body>
    <div class="title">EE290 Final Project</div>
    <h3 style="font-weight: normal; font-size: 1.5em;">Table of Contents</h3>
    <ul class="toc">
        <li><a href="#overview">Overview of Project</a></li>
        <li><a href="#data-generation">Data Generation</a></li>
        <li><a href="#SVM">Why not use a normal SVM?</a></li>
        <li><a href="#NN">Naive NN</a></li>
    </ul>

    <h4>Project Inspiration: <a href="https://ieeexplore.ieee.org/document/9024543" target="_blank">https://ieeexplore.ieee.org/document/9024543</a></h4>

    <h4 style="text-align: center">Abstract</h4>

    <p>
        Millimeter-wave (mmWave) small cell networks play an important role in 5G wireless communication systems. 
        By densely deploying a large number of mmWave small cell base stations (SBSs), thousands of connections 
        and high transmission rates are supported to provide a variety of local services [1], [2]. 
        The SBS provides short-range communications to mobile terminals (MTs) to reduce the propagation loss of 
        signal transmission [3]. With the help of mmWave, multiple SBSs can utilize a large number of antennas 
        to form directional analog beams to MTs and provide concurrent transmissions simultaneously. However, 
        as the number of SBSs and MTs increases, it becomes increasingly difficult to use traditional signal 
        processing methods to improve performance.
    </p>

    <p>
        For our project, we attempt to utilize machine learning (ML) to show a novel ML-based method for concurrent transmission in mmWave small cell networks.
    </p>

    <p>In the original paper, the authors follow a 3-step process:</p>
    <ul>
        <li>
            <strong>1. Random Distribution Modeling:</strong> 
            The large random distribution of SBSs is modeled by a heterogeneous Poisson point process (HPPP), 
            where the average sum rate (ASR) of MT under concurrent transmission can be obtained.
        </li>
        <li>
            <strong>2. Machine Learning for Beam Selection:</strong> 
            Downlink SBS conditions are established as a large database for machine learning training. An iterative 
            support vector machine (SVM) classifier is proposed for analog beam selection of each SBS.
        </li>
        <li>
            <strong>3. Iterative SMO Algorithm:</strong> 
            An iterative sequential minimal optimization (SMO) training algorithm is proposed, enabling SBSs to perform 
            highly efficient and low-complexity analog beam selection during concurrent transmissions.
        </li>
    </ul>

    <p>
        This process runs faster than traditional 
        <a href="https://www.mathworks.com/help/lte/ug/channel-estimation.html" target="_blank">channel estimation algorithms</a>
        on average.
    </p>

    <h4 style="text-decoration: underline;">For our project, we want to compare a naive Neural Network (NN) implementation, the paper's SMO algorithm, and a more complex NN implementation.</h4>

    <h2> Terminology </h2>
    
    <ul>
      <li>
          <strong>1. CodeBook :</strong> 
          <p>
            A <strong>codebook</strong> in beamforming is a collection of predefined 
            <strong>candidate beamforming vectors</strong>. Each vector represents a specific configuration 
            of phase shifts across the antennas of the SBS. These configurations determine the direction 
            and focus of the transmitted signal.
        </p>
        <h2>Mathematical Definition</h2>
        <p>The codebook $C$ contains $N_C$ candidate vectors,
        \(C = \{ c_{1}^S, c_{2}^S, \dots , c_{N_C}^S \} \), where <p style="text-align: center";>
            \(c_i^S \in \mathbb{C}^{N_{SBS} \times 1} \text{ for } i = 1, 2, \dots , N_C\)

        </p>
        <ul>
            <li>
                Each $c_i^{S}$ is a normalized beamforming vector:
                <p style="text-align: center";>
                    $c_i^S = \frac{1}{\sqrt{N_{SBS}}} \cdot [e^{j\phi_1}, e^{j\phi_2}, \dots , e^{j\phi_{N_{SBS}}}]^\top$
                </p>
            </li>
            $\phi_k$: Phase shift for the k-th antenna.</li> <br>
            $N_{SBS}$: Number of antennas at the SBS.</li>
        </ul>
        <p>
            Each vector in the codebook points the beam in a different direction, enabling the SBS to focus 
            its transmission on specific targets (MTs).
        </p>
      </li>
  </ul>

    <h2 id="data-generation">Data Generation</h2>

    <p>The first <i>(and typically most important)</i> step of any ML-based project is to collect lots of <a href="https://www.linkedin.com/pulse/what-good-quality-training-dataset-machine-learning-tagx" target="_blank">good</a> data.</p>

    <p> Problem is... we don't have access to this data, and most papers on beamforming do not share their data.
      As a result, we had to implement our own data generation script that follows the paper's implementaiton.

      This meant curating a large dataset that followed a HPPP. Here are the system parameters:

      <ul>
        <li>$\lambda_S = 1 \cdot 10^{-4}m^{-2}$: The density of our homogenous Poisson point process.</li>
        <li>$P_S = 20$ dBm: The maximum SPS power.</li>
        <li>$L = 2$: The number of propagation paths.</li>
        <li>$N_{MT} = 2$: The number of MT antennas.</li>
        <li>$N_{SBS} = 32$: The number of SBS antennas.</li>
        <li>$R = 100$m: The maximum MT communication radius.</li>
        <li>$N_C = 9$: The number of candidate vectors.</li>
        <li>$f = 28$ GHz: Carrier frequency</li>
        <li>$λ = c/f$: Wavelength</li>
        <li>$k = 2π/λ$: Wave number</li>
        <li>$D_MT = D_SBS = λ/2$: Antenna spacing</li>

      </ul>
    </p>

    In addition to that, we can now derive that the average number of SBS's in a circular area around our MT is 

    <p style="text-align: center">$N_{\mathrm{S}}=\lfloor\lambda_{\mathrm{S}}\pi R^{2}\rfloor$</p>

    Furthermore, if we define the data stream from the $k$th SBS to our MT as $d_{S, k}$, where $1 \leq k \leq N_S$, and the transmit power of our SBS as $P_{S, k}$, then our downlink signal of our SBS can be derived as 

    <p style="text-align: center">$s_{\mathrm{S}\mathrm{B}\mathrm{S},k} = c_{S, k}d_{S, k}, \text{where } c_{S, k} \in \mathbb{C}^{N_{\mathrm{S}\mathrm{B}\mathrm{S}}\times 1}$</p>

    <p> where $c_{S, k}$ is the $k$th SBS's analog beam (which directly points to the MT through the use of phase shifters). Our channel propagation for the $k$th SBS is actually based on the <a href="https://telcomatraining.com/what-is-sv-saleh-valenzuela/" target="_blank">Saleh-Valenzudela model</a>, a narrow band clustered channel model:</p>

    <p style="text-align: center">$\mathrm{H}_{\mathrm{S},k}=\gamma\sum_{l=1}^{L}\alpha_{\mathrm{S},k,l}\mathrm{a}_{\mathrm{M}\mathrm{T}}(\phi_{\mathrm{M}\mathrm{T},k,l})[\mathrm{a}_{\mathrm{S},k}(\emptyset \mathrm{s},k,l)]^{H}$</p>

    <p>where $\gamma = \sqrt{\frac{N_{SBS}N_{MT}}{L}}$, and $\alpha_{\mathrm{M}\mathrm{T}}$ is the complex gain from the $l$th path.</p>

    <body>
        <div class="algorithm">
            <div class="algorithm-title">Algorithm 1: mmWave Dataset Generation with Beam Selection</div>
            <div class="algorithm-block">
                1. Calculate number of SBS:<br>
                <div class="equation">N_S = ⌊λ_S π R²⌋</div>
                
                2. Generate DFT-based codebook C ∈ ℂ^(N_SBS × N_C):<br>
                <div class="algorithm-block">
                    For i = 0 to N_C - 1:<br>
                    <div class="equation">θᵢ = -π/2 + (i · π)/N_C</div>
                    <div class="equation">cᵢ = [1, e^(jkD_SBS sin(θᵢ)), ..., e^(jkD_SBS(N_SBS-1)sin(θᵢ))]^T/√N_SBS</div>
                </div>
                
                3. Generate channel matrix H for each sample:<br>
                <div class="algorithm-block">
                    For each path l = 1 to L:<br>
                    <div class="equation">φ_AOA,l ~ U(-π/2, π/2)  // Angle of Arrival</div>
                    <div class="equation">φ_AOD,l ~ U(-π/2, π/2)  // Angle of Departure</div>
                    <div class="equation">α_l ~ CN(0, 1/√2)       // Complex path gain</div>
                    <div class="equation">a_MT,l = [1, e^(jkD_MT sin(φ_AOA,l)), ..., e^(jkD_MT(N_MT-1)sin(φ_AOA,l))]^T/√N_MT</div>
                    <div class="equation">a_SBS,l = [1, e^(jkD_SBS sin(φ_AOD,l)), ..., e^(jkD_SBS(N_SBS-1)sin(φ_AOD,l))]^T/√N_SBS</div>
                </div>
                
                4. Construct complete channel matrix H:<br>
                <div class="equation">γ = √(N_SBS · N_MT/L)</div>
                <div class="equation">H = γ ∑(l=1 to L) α_l · a_MT,l · a_SBS,l^H</div>
                
                5. Calculate optimal beam index for each channel:<br>
                <div class="algorithm-block">
                    For each beam i in codebook:<br>
                    <div class="equation">SNR_i = (P_S · ‖H · cᵢ‖²)/N₀</div>
                    <div class="equation">optimal_beam = argmax_i(SNR_i)</div>
                </div>
                
                6. Feature vector construction for each sample:<br>
                <div class="equation">
                    x = [ φ_AOA,1, ..., φ_AOA,L,           // AoA features<br>
                         φ_AOD,1, ..., φ_AOD,L,           // AoD features<br>
                         Re{α_1}, ..., Re{α_L},           // Real path gains<br>
                         Im{α_1}, ..., Im{α_L},           // Imaginary path gains<br>
                         vec(Re{H}), vec(Im{H}) ]         // Vectorized channel matrix
                </div>
            </div>
    
            <div class="keyword">Output:</div>
            <div class="algorithm-block">
                Features matrix X ∈ ℝ^(N_samples × N_features)<br>
                Labels vector y ∈ ℤ^(N_samples), where yᵢ ∈ [0, N_C-1]
            </div>
    
            <div class="keyword">Notes:</div>
            <div class="algorithm-block">
                1. The noise power N₀ is set to 10⁻¹³ W<br>
                2. Channel matrix H ∈ ℂ^(N_MT × N_SBS)<br>
                3. Each codebook vector cᵢ is normalized: ‖cᵢ‖² = 1<br>
                4. Path gains α_l follow complex normal distribution<br>
                5. Feature vector dimension: N_features = 2L + 2L + 2(N_MT · N_SBS)
            </div>
        </div>
    </body>

  <h2 id="SVM">Why not use a normal SVM?</h2>

  <p> One of the main problems we face here is that small base stations are very <b> densely</b> deployed, and their placements are changing with every snapshot. </p>

  <p>This leads us to create training samples that use multiple snapshots of SBSs with the same density, as it allows us to improve our variance in our samples.</p>

  <p>Each training sample will contain the following, assuming we have $L$ propagation paths: </p>

  <ul>
    <li>Transmit power</li>
    <li>Path loss</li>
    <li>$2L$ <a href="https://en.wikipedia.org/wiki/Azimuth" target="_blank">azimuth angles</a>, one for the angle of arrival and one for the angle of departure for each path.</li>
    <li>$2L$ real and imaginary parts for the <a href="https://en.wikipedia.org/wiki/Complex_gain" target="_blank">complex gain</a>.</li>
  </ul>

  <p>We normalize our samples in order to prevent any inconsistencies from differing range values. For instance, all angles will be between $0$ and $2\pi$, all power will be measured in dB, etc.</p>

  <p>In total, every sample will be formatted as a vector of dimension $1 \times (4L + 2)$.</p>

  <p>If we were to use a traditional SVM, then we'd end up using $N_C$ separate classifiers in order to find hyperplanes that can cleanly separate our data. However, our data can be <b>incredibly imbalanced</b> (such as some beams having much fewer samples than other beams), which can introduce heavy bias and inaccurate predictions.</p>

  <p>Therefore, by using a <b>data-driven iterative SVM classifier</b>, we can better address the issue of imbalanced data and ensure better prediction accuracy for analog beam selection.</p> Specifically, we can perform resampling/reweighting with our samples deploy a more iterative process with each learning step to improve refinement.

  <h2 id="SMV Classifier">Data-Driven Iterative SVM classifier</h2>

  <p> We make use of the <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization" target="_blank">Sequential Minimal Optimization algorithm</a>, which solves the <a href="https://en.wikipedia.org/wiki/Quadratic_programming" target="_blank">quadratic programming</a> problem that comes up when training SVM's.</p>

  <p>We first choose a subset of two candidate vectors from our set of $N_C$ candidate vectors, and use these two vectors to classify our training samples into two groups.</p>

  <p>After each classification, one vector from our chosen two vectors is replaced by a new candidate vector not choosed before, and we repeat this process. In fact, this process will continue for $N_C - 1$ iterations (because by then each of our $N_C$ candidate vectors would have been chosen), making use of the SMO algorithm to train our SVM at each iteration.</p>


</html>
